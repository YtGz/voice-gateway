{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom openWakeWord Model\n",
    "\n",
    "Windows-compatible version of the openWakeWord training notebook.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Setup the Jupyter kernel (one-time)\n",
    "\n",
    "```bash\n",
    "uv add ipykernel --dev\n",
    "uv run python -m ipykernel install --user --name voice-gateway\n",
    "```\n",
    "\n",
    "### Run in VS Code\n",
    "\n",
    "1. Open this notebook in VS Code\n",
    "2. Click **Select Kernel** (top right) â†’ **voice-gateway**\n",
    "3. Run the cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True for NVIDIA GPU support (requires CUDA installed)\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install PyTorch with CUDA support from PyTorch's index\n",
    "if USE_GPU:\n",
    "    print(\"Installing PyTorch with CUDA support...\")\n",
    "    subprocess.run([\n",
    "        \"uv\", \"pip\", \"install\",\n",
    "        \"torch>=2.0,<2.6\", \"torchaudio\",\n",
    "        \"--index-url\", \"https://download.pytorch.org/whl/cu124\"\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Installing PyTorch (CPU only)...\")\n",
    "    subprocess.run([\"uv\", \"pip\", \"install\", \"torch>=2.0,<2.6\", \"torchaudio\"], check=True)\n",
    "\n",
    "# Verify GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies (Windows-compatible)\n",
    "deps = [\n",
    "    \"piper-tts>=1.2.0\",\n",
    "    \"numpy\",\n",
    "    \"scipy\",\n",
    "    \"tqdm\",\n",
    "    \"datasets==2.14.6\",\n",
    "    \"pyyaml\",\n",
    "    \"onnxruntime\",\n",
    "    \"onnx\",\n",
    "    \"pronouncing\",\n",
    "    \"deep-phonemizer\",\n",
    "    \"mutagen\",\n",
    "    \"torchinfo\",\n",
    "    \"torchmetrics\",\n",
    "    \"speechbrain==0.5.14\",\n",
    "    \"requests\",\n",
    "    \"ipywidgets\",\n",
    "]\n",
    "\n",
    "# Install dependencies\n",
    "failed = []\n",
    "for dep in deps:\n",
    "    result = subprocess.run([\"uv\", \"pip\", \"install\", dep], capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        failed.append(dep)\n",
    "        print(f\"Failed: {dep}\")\n",
    "    else:\n",
    "        print(f\"Installed: {dep}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nWarning: Failed to install: {failed}\")\n",
    "else:\n",
    "    print(\"\\nAll dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import wave\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "SCRIPT_DIR = Path(\".\").resolve()\n",
    "print(f\"Working directory: {SCRIPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download piper voice model (ONNX format - works on Windows)\n",
    "PIPER_MODELS_DIR = SCRIPT_DIR / \"piper_models\"\n",
    "PIPER_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"en_US-libritts_r-medium\"\n",
    "MODEL_PATH = PIPER_MODELS_DIR / f\"{MODEL_NAME}.onnx\"\n",
    "CONFIG_PATH = PIPER_MODELS_DIR / f\"{MODEL_NAME}.onnx.json\"\n",
    "\n",
    "base_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/libritts_r/medium\"\n",
    "\n",
    "for filepath, filename in [(MODEL_PATH, f\"{MODEL_NAME}.onnx\"), (CONFIG_PATH, f\"{MODEL_NAME}.onnx.json\")]:\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        url = f\"{base_url}/{filename}\"\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=filename) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone openwakeword\n",
    "OWW_DIR = SCRIPT_DIR / \"openwakeword\"\n",
    "\n",
    "if not OWW_DIR.exists():\n",
    "    print(\"Cloning openwakeword...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/dscripka/openwakeword\"], cwd=SCRIPT_DIR, check=True)\n",
    "else:\n",
    "    print(\"openwakeword already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download openwakeword embedding models\n",
    "OWW_MODELS_DIR = OWW_DIR / \"openwakeword\" / \"resources\" / \"models\"\n",
    "OWW_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_urls = {\n",
    "    \"embedding_model.onnx\": \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx\",\n",
    "    \"melspectrogram.onnx\": \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx\",\n",
    "}\n",
    "\n",
    "for filename, url in model_urls.items():\n",
    "    filepath = OWW_MODELS_DIR / filename\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Wake Word Pronunciation\n",
    "\n",
    "Before training, verify the TTS pronounces your wake word correctly.\n",
    "\n",
    "**Tips:**\n",
    "- If pronunciation is wrong, spell it phonetically with underscores: `\"hey_seer_e\"` for \"hey siri\"\n",
    "- Spell out numbers: `\"two\"` not `\"2\"`\n",
    "- Avoid punctuation except `?` and `!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your wake word here!\n",
    "TARGET_WORD = \"Seraphina\"  # Change this to your desired wake word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piper import PiperVoice\n",
    "from piper.config import SynthesisConfig\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Load the voice model\n",
    "voice = PiperVoice.load(str(MODEL_PATH), str(CONFIG_PATH))\n",
    "\n",
    "# Get number of speakers\n",
    "with open(CONFIG_PATH) as f:\n",
    "    voice_config = json.load(f)\n",
    "num_speakers = voice_config.get(\"num_speakers\", 1)\n",
    "print(f\"Voice model loaded with {num_speakers} speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(text: str, output_path: Path, speaker_id: int = 0, \n",
    "                    length_scale: float = 1.0, noise_scale: float = 0.667, \n",
    "                    noise_w_scale: float = 0.8):\n",
    "    \"\"\"Generate a single audio sample using piper-tts.\"\"\"\n",
    "    config = SynthesisConfig(\n",
    "        speaker_id=speaker_id,\n",
    "        length_scale=length_scale,\n",
    "        noise_scale=noise_scale,\n",
    "        noise_w_scale=noise_w_scale,\n",
    "    )\n",
    "    \n",
    "    with wave.open(str(output_path), \"wb\") as wav_file:\n",
    "        voice.synthesize_wav(text, wav_file, syn_config=config)\n",
    "\n",
    "# Test pronunciation\n",
    "test_path = SCRIPT_DIR / \"test_generation.wav\"\n",
    "generate_sample(TARGET_WORD, test_path, speaker_id=0, length_scale=1.1)\n",
    "display(Audio(str(test_path), autoplay=True))\n",
    "print(f\"\\nTest audio saved to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different speakers to hear variations\n",
    "print(\"Generating samples with different speakers...\")\n",
    "for speaker_id in range(min(5, num_speakers)):\n",
    "    sample_path = SCRIPT_DIR / f\"test_speaker_{speaker_id}.wav\"\n",
    "    generate_sample(TARGET_WORD, sample_path, speaker_id=speaker_id)\n",
    "    print(f\"Speaker {speaker_id}:\")\n",
    "    display(Audio(str(sample_path), autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Training Samples\n",
    "\n",
    "Generate diverse audio samples of the wake word using different speakers and synthesis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample configuration\n",
    "N_SAMPLES = 1000  # Number of samples to generate (more = better, but slower)\n",
    "SAMPLES_DIR = SCRIPT_DIR / \"generated_samples\" / TARGET_WORD.replace(\" \", \"_\")\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Will generate {N_SAMPLES} samples to: {SAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diverse training samples\n",
    "print(f\"Generating {N_SAMPLES} training samples...\")\n",
    "\n",
    "# Parameter ranges for variation\n",
    "length_scales = [0.8, 0.9, 1.0, 1.1, 1.2]  # Speech speed\n",
    "noise_scales = [0.5, 0.667, 0.8]  # Voice variation\n",
    "noise_w_scales = [0.6, 0.8, 1.0]  # Duration variation\n",
    "\n",
    "generated = 0\n",
    "for i in tqdm(range(N_SAMPLES), desc=\"Generating samples\"):\n",
    "    output_path = SAMPLES_DIR / f\"sample_{i:05d}.wav\"\n",
    "    \n",
    "    if output_path.exists():\n",
    "        generated += 1\n",
    "        continue\n",
    "    \n",
    "    # Random parameters for variation\n",
    "    speaker_id = random.randint(0, num_speakers - 1)\n",
    "    length_scale = random.choice(length_scales)\n",
    "    noise_scale = random.choice(noise_scales)\n",
    "    noise_w_scale = random.choice(noise_w_scales)\n",
    "    \n",
    "    try:\n",
    "        generate_sample(\n",
    "            TARGET_WORD, \n",
    "            output_path,\n",
    "            speaker_id=speaker_id,\n",
    "            length_scale=length_scale,\n",
    "            noise_scale=noise_scale,\n",
    "            noise_w_scale=noise_w_scale,\n",
    "        )\n",
    "        generated += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate sample {i}: {e}\")\n",
    "\n",
    "print(f\"\\nGenerated {generated} samples in {SAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Training Data\n",
    "\n",
    "Download pre-computed negative examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_LARGE_DOWNLOAD = False  # Set to True to skip the 16GB download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation features (small, always download)\n",
    "VAL_PATH = SCRIPT_DIR / \"validation_set_features.npy\"\n",
    "\n",
    "if not VAL_PATH.exists():\n",
    "    print(\"Downloading validation features...\")\n",
    "    url = \"https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "    with open(VAL_PATH, \"wb\") as f:\n",
    "        with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Validation features\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "else:\n",
    "    print(\"Validation features already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training features (large)\n",
    "FEATURES_PATH = SCRIPT_DIR / \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "\n",
    "if SKIP_LARGE_DOWNLOAD:\n",
    "    print(\"Skipping large feature download (training quality will be reduced)\")\n",
    "elif not FEATURES_PATH.exists():\n",
    "    print(\"Downloading training features (16GB, this will take a while)...\")\n",
    "    url = \"https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "    with open(FEATURES_PATH, \"wb\") as f:\n",
    "        with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Training features\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "else:\n",
    "    print(\"Training features already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Adjust these parameters:\n",
    "- `N_STEPS`: Training steps (10000 is quick, more is better)\n",
    "- `FALSE_ACTIVATION_PENALTY`: Higher = fewer false activations but may miss quiet/noisy speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 10000\n",
    "FALSE_ACTIVATION_PENALTY = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load default config\n",
    "with open(OWW_DIR / \"examples\" / \"custom_model.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Modify config\n",
    "config[\"target_phrase\"] = [TARGET_WORD]\n",
    "config[\"model_name\"] = TARGET_WORD.replace(\" \", \"_\")\n",
    "config[\"custom_model_dir\"] = str(SAMPLES_DIR)\n",
    "config[\"n_samples\"] = N_SAMPLES\n",
    "config[\"n_samples_val\"] = max(100, N_SAMPLES // 10)\n",
    "config[\"steps\"] = N_STEPS\n",
    "config[\"target_accuracy\"] = 0.5\n",
    "config[\"target_recall\"] = 0.25\n",
    "config[\"output_dir\"] = str(SCRIPT_DIR / \"my_custom_model\")\n",
    "config[\"max_negative_weight\"] = FALSE_ACTIVATION_PENALTY\n",
    "\n",
    "# Data paths\n",
    "config[\"background_paths\"] = []\n",
    "config[\"false_positive_validation_data_path\"] = str(VAL_PATH)\n",
    "\n",
    "if FEATURES_PATH.exists():\n",
    "    config[\"feature_data_files\"] = {\"ACAV100M_sample\": str(FEATURES_PATH)}\n",
    "else:\n",
    "    config[\"feature_data_files\"] = {}\n",
    "\n",
    "# Use pre-generated clips (skip generation step)\n",
    "config[\"custom_clips_dir\"] = str(SAMPLES_DIR)\n",
    "\n",
    "# Save config\n",
    "CONFIG_PATH = SCRIPT_DIR / \"my_model.yaml\"\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Config saved to: {CONFIG_PATH}\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Target word: {TARGET_WORD}\")\n",
    "print(f\"  Samples: {N_SAMPLES}\")\n",
    "print(f\"  Steps: {N_STEPS}\")\n",
    "print(f\"  Output: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "This runs the training pipeline. With default settings, this takes 30-60 minutes on CPU, faster on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SCRIPT = OWW_DIR / \"openwakeword\" / \"train.py\"\n",
    "\n",
    "# Skip clip generation since we already generated them\n",
    "print(\"=\" * 50)\n",
    "print(\"Step 1: Augmenting clips\")\n",
    "print(\"=\" * 50)\n",
    "subprocess.run([sys.executable, str(TRAIN_SCRIPT), \"--training_config\", str(CONFIG_PATH), \"--augment_clips\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Step 2: Training model\")\n",
    "print(\"=\" * 50)\n",
    "subprocess.run([sys.executable, str(TRAIN_SCRIPT), \"--training_config\", str(CONFIG_PATH), \"--train_model\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Done!\n",
    "\n",
    "Your trained model is in the `my_custom_model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(config[\"output_dir\"])\n",
    "print(f\"\\nTraining complete! Model files:\")\n",
    "if OUTPUT_DIR.exists():\n",
    "    for f in OUTPUT_DIR.glob(\"*\"):\n",
    "        print(f\"  {f}\")\n",
    "else:\n",
    "    print(f\"  Output directory not found: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Copy to wakewords folder\n",
    "import shutil\n",
    "\n",
    "WAKEWORD_DIR = SCRIPT_DIR.parent / \"wakewords\" / TARGET_WORD.lower().replace(\" \", \"_\")\n",
    "WAKEWORD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = TARGET_WORD.replace(\" \", \"_\")\n",
    "for ext in [\".onnx\", \".tflite\"]:\n",
    "    src = OUTPUT_DIR / f\"{model_name}{ext}\"\n",
    "    if src.exists():\n",
    "        dst = WAKEWORD_DIR / f\"{model_name}{ext}\"\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"Copied {src.name} to {dst}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
