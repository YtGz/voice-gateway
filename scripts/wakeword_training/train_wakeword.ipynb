{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom openWakeWord Model\n",
    "\n",
    "Windows-compatible version of the openWakeWord training notebook.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Setup the Jupyter kernel (one-time)\n",
    "\n",
    "```bash\n",
    "uv add ipykernel --dev\n",
    "uv run python -m ipykernel install --user --name voice-gateway\n",
    "```\n",
    "\n",
    "### Run in VS Code\n",
    "\n",
    "1. Open this notebook in VS Code\n",
    "2. Click **Select Kernel** (top right) â†’ **voice-gateway**\n",
    "3. Run the cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True for NVIDIA GPU support (requires CUDA installed)\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install PyTorch with CUDA support from PyTorch's index\n",
    "if USE_GPU:\n",
    "    print(\"Installing PyTorch with CUDA support...\")\n",
    "    subprocess.run([\n",
    "        \"uv\", \"pip\", \"install\",\n",
    "        \"torch>=2.0,<2.6\", \"torchaudio\",\n",
    "        \"--index-url\", \"https://download.pytorch.org/whl/cu124\"\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Installing PyTorch (CPU only)...\")\n",
    "    subprocess.run([\"uv\", \"pip\", \"install\", \"torch>=2.0,<2.6\", \"torchaudio\"], check=True)\n",
    "\n",
    "# Verify GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies (Windows-compatible)\n",
    "deps = [\n",
    "    \"piper-tts>=1.2.0\",\n",
    "    \"numpy\",\n",
    "    \"scipy\",\n",
    "    \"tqdm\",\n",
    "    \"datasets==2.14.6\",\n",
    "    \"pyyaml\",\n",
    "    \"onnxruntime\",\n",
    "    \"onnx\",\n",
    "    \"pronouncing\",\n",
    "    \"deep-phonemizer\",\n",
    "    \"mutagen\",\n",
    "    \"torchinfo\",\n",
    "    \"torchmetrics\",\n",
    "    \"speechbrain==0.5.14\",\n",
    "    \"requests\",\n",
    "    \"ipywidgets\",\n",
    "    \"soundfile\",\n",
    "]\n",
    "\n",
    "# Install dependencies\n",
    "failed = []\n",
    "for dep in deps:\n",
    "    result = subprocess.run([\"uv\", \"pip\", \"install\", dep], capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        failed.append(dep)\n",
    "        print(f\"Failed: {dep}\")\n",
    "    else:\n",
    "        print(f\"Installed: {dep}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nWarning: Failed to install: {failed}\")\n",
    "else:\n",
    "    print(\"\\nAll dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import wave\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import scipy.signal\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "SCRIPT_DIR = Path(\".\").resolve()\n",
    "print(f\"Working directory: {SCRIPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download piper voice model (ONNX format - works on Windows)\n",
    "PIPER_MODELS_DIR = SCRIPT_DIR / \"piper_models\"\n",
    "PIPER_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"en_US-libritts_r-medium\"\n",
    "PIPER_MODEL_PATH = PIPER_MODELS_DIR / f\"{MODEL_NAME}.onnx\"\n",
    "PIPER_CONFIG_PATH = PIPER_MODELS_DIR / f\"{MODEL_NAME}.onnx.json\"\n",
    "\n",
    "base_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/libritts_r/medium\"\n",
    "\n",
    "for filepath, filename in [(PIPER_MODEL_PATH, f\"{MODEL_NAME}.onnx\"), (PIPER_CONFIG_PATH, f\"{MODEL_NAME}.onnx.json\")]:\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        url = f\"{base_url}/{filename}\"\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=filename) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone openwakeword\n",
    "OWW_DIR = SCRIPT_DIR / \"openwakeword\"\n",
    "\n",
    "if not OWW_DIR.exists():\n",
    "    print(\"Cloning openwakeword...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/dscripka/openwakeword\"], cwd=SCRIPT_DIR, check=True)\n",
    "else:\n",
    "    print(\"openwakeword already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download openwakeword embedding models\n",
    "OWW_MODELS_DIR = OWW_DIR / \"openwakeword\" / \"resources\" / \"models\"\n",
    "OWW_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_urls = {\n",
    "    \"embedding_model.onnx\": \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx\",\n",
    "    \"melspectrogram.onnx\": \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx\",\n",
    "}\n",
    "\n",
    "for filename, url in model_urls.items():\n",
    "    filepath = OWW_MODELS_DIR / filename\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Augmentation Data\n",
    "\n",
    "Download room impulse responses (RIRs) and background noise for data augmentation.\n",
    "This significantly improves model quality by simulating real-world acoustic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "AUGMENT_DIR = SCRIPT_DIR / \"augmentation_data\"\n",
    "AUGMENT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RIR_DIR = AUGMENT_DIR / \"rir\"\n",
    "NOISE_DIR = AUGMENT_DIR / \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MIT Acoustics RIR dataset (small, ~50MB)\n",
    "RIR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Using the OpenSLR RIR dataset (simulated room impulse responses)\n",
    "RIR_URL = \"https://www.openslr.org/resources/28/rirs_noises.zip\"\n",
    "RIR_ZIP = AUGMENT_DIR / \"rirs_noises.zip\"\n",
    "\n",
    "if not (RIR_DIR / \"RIRS_NOISES\").exists():\n",
    "    if not RIR_ZIP.exists():\n",
    "        print(\"Downloading RIR dataset (~300MB)...\")\n",
    "        response = requests.get(RIR_URL, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(RIR_ZIP, \"wb\") as f:\n",
    "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"RIR dataset\") as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    \n",
    "    print(\"Extracting RIR dataset...\")\n",
    "    with zipfile.ZipFile(RIR_ZIP, 'r') as z:\n",
    "        z.extractall(RIR_DIR)\n",
    "    print(\"RIR dataset ready\")\n",
    "else:\n",
    "    print(\"RIR dataset already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MUSAN noise dataset (background noise, music, babble)\n",
    "NOISE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MUSAN_URL = \"https://www.openslr.org/resources/17/musan.tar.gz\"\n",
    "MUSAN_TAR = AUGMENT_DIR / \"musan.tar.gz\"\n",
    "\n",
    "if not (NOISE_DIR / \"musan\").exists():\n",
    "    if not MUSAN_TAR.exists():\n",
    "        print(\"Downloading MUSAN noise dataset (~11GB, this will take a while)...\")\n",
    "        response = requests.get(MUSAN_URL, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(MUSAN_TAR, \"wb\") as f:\n",
    "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"MUSAN dataset\") as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    \n",
    "    print(\"Extracting MUSAN dataset (this may take a few minutes)...\")\n",
    "    with tarfile.open(MUSAN_TAR, 'r:gz') as tar:\n",
    "        tar.extractall(NOISE_DIR)\n",
    "    print(\"MUSAN dataset ready\")\n",
    "else:\n",
    "    print(\"MUSAN dataset already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all RIR and noise file paths\n",
    "rir_paths = []\n",
    "for ext in [\"*.wav\", \"*.flac\"]:\n",
    "    rir_paths.extend(list((RIR_DIR / \"RIRS_NOISES\" / \"simulated_rirs\").rglob(ext)))\n",
    "    rir_paths.extend(list((RIR_DIR / \"RIRS_NOISES\" / \"real_rirs_isotropic_noises\").rglob(ext)))\n",
    "\n",
    "noise_paths = []\n",
    "musan_dir = NOISE_DIR / \"musan\"\n",
    "if musan_dir.exists():\n",
    "    # Include noise and music, skip speech (we want non-speech background)\n",
    "    noise_paths.extend(list((musan_dir / \"noise\").rglob(\"*.wav\")))\n",
    "    noise_paths.extend(list((musan_dir / \"music\").rglob(\"*.wav\")))\n",
    "\n",
    "print(f\"Found {len(rir_paths)} RIR files\")\n",
    "print(f\"Found {len(noise_paths)} noise/music files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Wake Word Pronunciation\n",
    "\n",
    "Before training, verify the TTS pronounces your wake word correctly.\n",
    "\n",
    "**Tips:**\n",
    "- If pronunciation is wrong, spell it phonetically with underscores: `\"hey_seer_e\"` for \"hey siri\"\n",
    "- Spell out numbers: `\"two\"` not `\"2\"`\n",
    "- Avoid punctuation except `?` and `!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your wake word here!\n",
    "TARGET_WORD = \"Seraphina\"  # Change this to your desired wake word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piper import PiperVoice\n",
    "from piper.config import SynthesisConfig\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Load the voice model\n",
    "voice = PiperVoice.load(str(PIPER_MODEL_PATH), str(PIPER_CONFIG_PATH))\n",
    "\n",
    "# Get number of speakers\n",
    "with open(PIPER_CONFIG_PATH, encoding=\"utf-8\") as f:\n",
    "    voice_config = json.load(f)\n",
    "num_speakers = voice_config.get(\"num_speakers\", 1)\n",
    "print(f\"Voice model loaded with {num_speakers} speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(text: str, output_path: Path, speaker_id: int = 0, \n",
    "                    length_scale: float = 1.0, noise_scale: float = 0.667, \n",
    "                    noise_w_scale: float = 0.8):\n",
    "    \"\"\"Generate a single audio sample using piper-tts.\"\"\"\n",
    "    config = SynthesisConfig(\n",
    "        speaker_id=speaker_id,\n",
    "        length_scale=length_scale,\n",
    "        noise_scale=noise_scale,\n",
    "        noise_w_scale=noise_w_scale,\n",
    "    )\n",
    "    \n",
    "    with wave.open(str(output_path), \"wb\") as wav_file:\n",
    "        voice.synthesize_wav(text, wav_file, syn_config=config)\n",
    "\n",
    "# Test pronunciation\n",
    "test_path = SCRIPT_DIR / \"test_generation.wav\"\n",
    "generate_sample(TARGET_WORD, test_path, speaker_id=0, length_scale=1.1)\n",
    "display(Audio(str(test_path), autoplay=True))\n",
    "print(f\"\\nTest audio saved to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different speakers to hear variations\n",
    "print(\"Generating samples with different speakers...\")\n",
    "for speaker_id in range(min(5, num_speakers)):\n",
    "    sample_path = SCRIPT_DIR / f\"test_speaker_{speaker_id}.wav\"\n",
    "    generate_sample(TARGET_WORD, sample_path, speaker_id=speaker_id)\n",
    "    print(f\"Speaker {speaker_id}:\")\n",
    "    display(Audio(str(sample_path), autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Samples\n",
    "\n",
    "Generate diverse audio samples of the wake word using different speakers and synthesis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample configuration\n",
    "N_SAMPLES_TRAIN = 2000  # Training samples (more = better, but slower)\n",
    "N_SAMPLES_TEST = 200    # Test/validation samples\n",
    "AUGMENTATION_ROUNDS = 3  # How many augmented versions of each sample to create\n",
    "\n",
    "# Output directories matching openwakeword's expected structure\n",
    "MODEL_OUTPUT_NAME = TARGET_WORD.replace(\" \", \"_\")\n",
    "OUTPUT_DIR = SCRIPT_DIR / \"my_custom_model\"\n",
    "MODEL_DIR = OUTPUT_DIR / MODEL_OUTPUT_NAME\n",
    "\n",
    "POSITIVE_TRAIN_DIR = MODEL_DIR / \"positive_train\"\n",
    "POSITIVE_TEST_DIR = MODEL_DIR / \"positive_test\"\n",
    "NEGATIVE_TRAIN_DIR = MODEL_DIR / \"negative_train\"\n",
    "NEGATIVE_TEST_DIR = MODEL_DIR / \"negative_test\"\n",
    "\n",
    "for d in [POSITIVE_TRAIN_DIR, POSITIVE_TEST_DIR, NEGATIVE_TRAIN_DIR, NEGATIVE_TEST_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_batch(text: str, output_dir: Path, n_samples: int, desc: str = \"Generating\"):\n",
    "    \"\"\"Generate diverse training samples.\"\"\"\n",
    "    # Parameter ranges for variation\n",
    "    length_scales = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "    noise_scales = [0.5, 0.667, 0.8]\n",
    "    noise_w_scales = [0.6, 0.8, 1.0]\n",
    "    \n",
    "    generated = 0\n",
    "    for i in tqdm(range(n_samples), desc=desc):\n",
    "        output_path = output_dir / f\"sample_{i:05d}.wav\"\n",
    "        \n",
    "        if output_path.exists():\n",
    "            generated += 1\n",
    "            continue\n",
    "        \n",
    "        speaker_id = random.randint(0, num_speakers - 1)\n",
    "        length_scale = random.choice(length_scales)\n",
    "        noise_scale = random.choice(noise_scales)\n",
    "        noise_w_scale = random.choice(noise_w_scales)\n",
    "        \n",
    "        try:\n",
    "            generate_sample(\n",
    "                text, output_path,\n",
    "                speaker_id=speaker_id,\n",
    "                length_scale=length_scale,\n",
    "                noise_scale=noise_scale,\n",
    "                noise_w_scale=noise_w_scale,\n",
    "            )\n",
    "            generated += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed sample {i}: {e}\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate positive samples\n",
    "print(f\"\\nGenerating {N_SAMPLES_TRAIN} positive training samples...\")\n",
    "n = generate_samples_batch(TARGET_WORD, POSITIVE_TRAIN_DIR, N_SAMPLES_TRAIN, \"Positive train\")\n",
    "print(f\"Generated {n} training samples\")\n",
    "\n",
    "print(f\"\\nGenerating {N_SAMPLES_TEST} positive test samples...\")\n",
    "n = generate_samples_batch(TARGET_WORD, POSITIVE_TEST_DIR, N_SAMPLES_TEST, \"Positive test\")\n",
    "print(f\"Generated {n} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial negative samples (similar-sounding words)\n",
    "# Inlined from openwakeword.data to avoid audiomentations dependency\n",
    "import itertools\n",
    "import re\n",
    "import pronouncing\n",
    "\n",
    "def phoneme_replacement(input_chars, max_replace, replace_char='(.){1,3}'):\n",
    "    \"\"\"Generate regex patterns with phonemes replaced.\"\"\"\n",
    "    results = []\n",
    "    chars = list(input_chars)\n",
    "    for r in range(1, max_replace + 1):\n",
    "        comb = itertools.combinations(range(len(chars)), r)\n",
    "        for indices in comb:\n",
    "            chars_copy = chars.copy()\n",
    "            for i in indices:\n",
    "                chars_copy[i] = replace_char\n",
    "            results.append(' '.join(chars_copy))\n",
    "    return results\n",
    "\n",
    "def generate_adversarial_texts(input_text: str, N: int, include_partial_phrase: float = 0, include_input_words: float = 0):\n",
    "    \"\"\"Generate adversarial words based on phoneme overlap.\"\"\"\n",
    "    vowel_phones = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AX\", \"AXR\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\", \"IX\", \"IY\", \"OW\", \"OY\", \"UH\", \"UW\", \"UX\"]\n",
    "    \n",
    "    word_phones = []\n",
    "    input_text_phones = [pronouncing.phones_for_word(i) for i in input_text.split()]\n",
    "    \n",
    "    # Handle OOV words with simple fallback\n",
    "    for phones, word in zip(input_text_phones, input_text.split()):\n",
    "        if phones:\n",
    "            word_phones.append(phones[0] if isinstance(phones[0], str) else phones[0])\n",
    "        else:\n",
    "            # Simple fallback for OOV words - use the word itself\n",
    "            print(f\"Warning: '{word}' not in pronunciation dictionary, using simple phoneme approximation\")\n",
    "            word_phones.append(word.upper())\n",
    "    \n",
    "    # Add lexical stress variants to vowels\n",
    "    word_phones = [re.sub('|'.join(vowel_phones), lambda x: str(x.group(0)) + '[0|1|2]', re.sub(r'\\\\d+', '', i)) for i in word_phones]\n",
    "    \n",
    "    adversarial_phrases = []\n",
    "    for phones, word in zip(word_phones, input_text.split()):\n",
    "        query_exps = []\n",
    "        phones_list = phones.split()\n",
    "        adversarial_words = []\n",
    "        if len(phones_list) <= 2:\n",
    "            query_exps.append(\" \".join(phones_list))\n",
    "        else:\n",
    "            query_exps.extend(phoneme_replacement(phones_list, max_replace=max(0, len(phones_list)-2), replace_char=\"(.){1,3}\"))\n",
    "        \n",
    "        for query in query_exps:\n",
    "            try:\n",
    "                matches = pronouncing.search(query)\n",
    "                matches_phones = [pronouncing.phones_for_word(i)[0] for i in matches if pronouncing.phones_for_word(i)]\n",
    "                allowed_matches = [i for i, j in zip(matches, matches_phones) if j != phones]\n",
    "                adversarial_words.extend([i for i in allowed_matches if word.lower() != i])\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        if adversarial_words:\n",
    "            adversarial_phrases.append(adversarial_words)\n",
    "    \n",
    "    # Build combinations\n",
    "    adversarial_texts = []\n",
    "    if not adversarial_phrases:\n",
    "        return adversarial_texts\n",
    "    \n",
    "    for i in range(N):\n",
    "        txts = []\n",
    "        for j, k in zip(adversarial_phrases, input_text.split()):\n",
    "            if np.random.random() > (1 - include_input_words):\n",
    "                txts.append(k)\n",
    "            else:\n",
    "                txts.append(np.random.choice(j))\n",
    "        \n",
    "        if include_partial_phrase and len(input_text.split()) > 1 and np.random.random() <= include_partial_phrase:\n",
    "            n_words = np.random.randint(1, len(input_text.split()) + 1)\n",
    "            adversarial_texts.append(\" \".join(np.random.choice(txts, size=n_words, replace=False)))\n",
    "        else:\n",
    "            adversarial_texts.append(\" \".join(txts))\n",
    "    \n",
    "    # Remove exact matches\n",
    "    adversarial_texts = [i for i in adversarial_texts if i != input_text]\n",
    "    return adversarial_texts\n",
    "\n",
    "# Generate adversarial texts\n",
    "adversarial_texts = []\n",
    "for word in TARGET_WORD.split():\n",
    "    adversarial_texts.extend(generate_adversarial_texts(\n",
    "        input_text=word,\n",
    "        N=N_SAMPLES_TRAIN // max(1, len(TARGET_WORD.split())),\n",
    "        include_partial_phrase=0.5,\n",
    "        include_input_words=0.2\n",
    "    ))\n",
    "\n",
    "print(f\"Generated {len(adversarial_texts)} adversarial phrases\")\n",
    "print(f\"Examples: {adversarial_texts[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative training samples\n",
    "print(f\"\\nGenerating {N_SAMPLES_TRAIN} negative training samples...\")\n",
    "\n",
    "length_scales = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "noise_scales = [0.5, 0.667, 0.8]\n",
    "noise_w_scales = [0.6, 0.8, 1.0]\n",
    "\n",
    "generated = 0\n",
    "for i in tqdm(range(N_SAMPLES_TRAIN), desc=\"Negative train\"):\n",
    "    output_path = NEGATIVE_TRAIN_DIR / f\"sample_{i:05d}.wav\"\n",
    "    if output_path.exists():\n",
    "        generated += 1\n",
    "        continue\n",
    "    \n",
    "    text = random.choice(adversarial_texts)\n",
    "    speaker_id = random.randint(0, num_speakers - 1)\n",
    "    \n",
    "    try:\n",
    "        generate_sample(\n",
    "            text, output_path,\n",
    "            speaker_id=speaker_id,\n",
    "            length_scale=random.choice(length_scales),\n",
    "            noise_scale=random.choice(noise_scales),\n",
    "            noise_w_scale=random.choice(noise_w_scales),\n",
    "        )\n",
    "        generated += 1\n",
    "    except Exception as e:\n",
    "        pass  # Some adversarial texts may fail, that's ok\n",
    "\n",
    "print(f\"Generated {generated} negative training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative test samples\n",
    "print(f\"\\nGenerating {N_SAMPLES_TEST} negative test samples...\")\n",
    "\n",
    "generated = 0\n",
    "for i in tqdm(range(N_SAMPLES_TEST), desc=\"Negative test\"):\n",
    "    output_path = NEGATIVE_TEST_DIR / f\"sample_{i:05d}.wav\"\n",
    "    if output_path.exists():\n",
    "        generated += 1\n",
    "        continue\n",
    "    \n",
    "    text = random.choice(adversarial_texts)\n",
    "    speaker_id = random.randint(0, num_speakers - 1)\n",
    "    \n",
    "    try:\n",
    "        generate_sample(\n",
    "            text, output_path,\n",
    "            speaker_id=speaker_id,\n",
    "            length_scale=random.choice(length_scales),\n",
    "            noise_scale=random.choice(noise_scales),\n",
    "            noise_w_scale=random.choice(noise_w_scales),\n",
    "        )\n",
    "        generated += 1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f\"Generated {generated} negative test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation (Windows-Compatible)\n",
    "\n",
    "Apply audio augmentations to simulate real-world conditions:\n",
    "- Room reverb (RIR convolution)\n",
    "- Background noise mixing\n",
    "- Volume variation\n",
    "- Pitch shifting\n",
    "- Speed variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path: Path, target_sr: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Load audio file and resample to target sample rate.\"\"\"\n",
    "    waveform, sr = torchaudio.load(str(path))\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    return waveform.squeeze(0)\n",
    "\n",
    "\n",
    "def apply_rir(waveform: torch.Tensor, rir: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply room impulse response via convolution.\"\"\"\n",
    "    # Normalize RIR\n",
    "    rir = rir / (rir.abs().max() + 1e-8)\n",
    "    # Convolve\n",
    "    result = torch.nn.functional.conv1d(\n",
    "        waveform.unsqueeze(0).unsqueeze(0),\n",
    "        rir.unsqueeze(0).unsqueeze(0),\n",
    "        padding=rir.shape[0] // 2\n",
    "    ).squeeze()\n",
    "    # Trim to original length\n",
    "    return result[:waveform.shape[0]]\n",
    "\n",
    "\n",
    "def mix_audio(foreground: torch.Tensor, background: torch.Tensor, snr_db: float) -> torch.Tensor:\n",
    "    \"\"\"Mix foreground with background at specified SNR.\"\"\"\n",
    "    # Ensure background is long enough\n",
    "    while background.shape[0] < foreground.shape[0]:\n",
    "        background = torch.cat([background, background])\n",
    "    \n",
    "    # Random start position in background\n",
    "    max_start = background.shape[0] - foreground.shape[0]\n",
    "    if max_start > 0:\n",
    "        start = random.randint(0, max_start)\n",
    "        background = background[start:start + foreground.shape[0]]\n",
    "    else:\n",
    "        background = background[:foreground.shape[0]]\n",
    "    \n",
    "    # Calculate scaling for desired SNR\n",
    "    fg_rms = foreground.pow(2).mean().sqrt() + 1e-8\n",
    "    bg_rms = background.pow(2).mean().sqrt() + 1e-8\n",
    "    snr_linear = 10 ** (snr_db / 20)\n",
    "    scale = fg_rms / (bg_rms * snr_linear)\n",
    "    \n",
    "    mixed = foreground + background * scale\n",
    "    # Normalize to prevent clipping\n",
    "    max_val = mixed.abs().max()\n",
    "    if max_val > 1.0:\n",
    "        mixed = mixed / max_val\n",
    "    \n",
    "    return mixed\n",
    "\n",
    "\n",
    "def create_fixed_length_clip(waveform: torch.Tensor, target_length: int, \n",
    "                              end_jitter_ms: int = 200, sr: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Pad/truncate clip to fixed length, with the clip near the end.\"\"\"\n",
    "    result = torch.zeros(target_length)\n",
    "    \n",
    "    # Random jitter for end position\n",
    "    end_jitter_samples = int(random.uniform(0, end_jitter_ms / 1000) * sr)\n",
    "    \n",
    "    if waveform.shape[0] >= target_length:\n",
    "        # Truncate - take from random position\n",
    "        if random.random() > 0.5:\n",
    "            result = waveform[:target_length]\n",
    "        else:\n",
    "            result = waveform[-target_length:]\n",
    "    else:\n",
    "        # Pad - position clip near end with jitter\n",
    "        start = max(0, target_length - waveform.shape[0] - end_jitter_samples)\n",
    "        end = start + waveform.shape[0]\n",
    "        result[start:end] = waveform\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def augment_clip(waveform: torch.Tensor, \n",
    "                 target_length: int,\n",
    "                 rir_paths: list,\n",
    "                 noise_paths: list,\n",
    "                 sr: int = 16000,\n",
    "                 rir_prob: float = 0.5,\n",
    "                 noise_prob: float = 0.75,\n",
    "                 pitch_shift_prob: float = 0.25,\n",
    "                 speed_prob: float = 0.25) -> torch.Tensor:\n",
    "    \"\"\"Apply augmentations to a single clip.\"\"\"\n",
    "    \n",
    "    # Speed perturbation (before padding)\n",
    "    if random.random() < speed_prob:\n",
    "        speed_factor = random.uniform(0.9, 1.1)\n",
    "        effects = [[\"speed\", str(speed_factor)], [\"rate\", str(sr)]]\n",
    "        waveform_np = waveform.numpy()\n",
    "        waveform, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "            torch.from_numpy(waveform_np).unsqueeze(0), sr, effects\n",
    "        )\n",
    "        waveform = waveform.squeeze(0)\n",
    "    \n",
    "    # Pitch shift\n",
    "    if random.random() < pitch_shift_prob:\n",
    "        n_steps = random.uniform(-3, 3)\n",
    "        waveform = torchaudio.functional.pitch_shift(waveform.unsqueeze(0), sr, n_steps).squeeze(0)\n",
    "    \n",
    "    # Create fixed-length clip\n",
    "    waveform = create_fixed_length_clip(waveform, target_length, sr=sr)\n",
    "    \n",
    "    # Apply RIR (reverb)\n",
    "    if rir_paths and random.random() < rir_prob:\n",
    "        try:\n",
    "            rir = load_audio(random.choice(rir_paths), sr)\n",
    "            waveform = apply_rir(waveform, rir)\n",
    "        except Exception:\n",
    "            pass  # Skip if RIR loading fails\n",
    "    \n",
    "    # Mix with background noise\n",
    "    if noise_paths and random.random() < noise_prob:\n",
    "        try:\n",
    "            noise = load_audio(random.choice(noise_paths), sr)\n",
    "            snr_db = random.uniform(-5, 20)  # Wide range of SNRs\n",
    "            waveform = mix_audio(waveform, noise, snr_db)\n",
    "        except Exception:\n",
    "            pass  # Skip if noise loading fails\n",
    "    \n",
    "    # Volume augmentation\n",
    "    volume = random.uniform(0.1, 1.0)\n",
    "    waveform = waveform * (volume / (waveform.abs().max() + 1e-8))\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "\n",
    "def augment_clips_generator(clip_paths: list,\n",
    "                            target_length: int,\n",
    "                            rir_paths: list,\n",
    "                            noise_paths: list,\n",
    "                            batch_size: int = 64,\n",
    "                            augmentation_rounds: int = 1,\n",
    "                            sr: int = 16000):\n",
    "    \"\"\"Generator that yields batches of augmented clips as 16-bit PCM.\"\"\"\n",
    "    \n",
    "    # Expand clip list for augmentation rounds\n",
    "    all_clips = clip_paths * augmentation_rounds\n",
    "    random.shuffle(all_clips)\n",
    "    \n",
    "    for i in range(0, len(all_clips), batch_size):\n",
    "        batch_paths = all_clips[i:i + batch_size]\n",
    "        batch = []\n",
    "        \n",
    "        for clip_path in batch_paths:\n",
    "            try:\n",
    "                waveform = load_audio(clip_path, sr)\n",
    "                augmented = augment_clip(\n",
    "                    waveform, target_length, rir_paths, noise_paths, sr\n",
    "                )\n",
    "                # Convert to 16-bit PCM\n",
    "                pcm = (augmented.numpy() * 32767).astype(np.int16)\n",
    "                batch.append(pcm)\n",
    "            except Exception as e:\n",
    "                continue  # Skip failed clips\n",
    "        \n",
    "        if batch:\n",
    "            yield np.stack(batch)\n",
    "\n",
    "print(\"Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine clip length from samples\n",
    "positive_clips = list(POSITIVE_TEST_DIR.glob(\"*.wav\"))\n",
    "durations = []\n",
    "for clip in positive_clips[:50]:\n",
    "    waveform = load_audio(clip)\n",
    "    durations.append(waveform.shape[0])\n",
    "\n",
    "median_duration = int(np.median(durations))\n",
    "# Add 750ms buffer and round to nearest 1000\n",
    "TOTAL_LENGTH = int(np.ceil((median_duration + 12000) / 1000) * 1000)\n",
    "# Minimum 2 seconds\n",
    "TOTAL_LENGTH = max(TOTAL_LENGTH, 32000)\n",
    "# Clamp near 32000 to exactly 32000\n",
    "if abs(TOTAL_LENGTH - 32000) <= 4000:\n",
    "    TOTAL_LENGTH = 32000\n",
    "\n",
    "print(f\"Median clip duration: {median_duration} samples ({median_duration/16000:.2f}s)\")\n",
    "print(f\"Target augmented length: {TOTAL_LENGTH} samples ({TOTAL_LENGTH/16000:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test augmentation on a single clip\n",
    "test_clip = list(POSITIVE_TRAIN_DIR.glob(\"*.wav\"))[0]\n",
    "waveform = load_audio(test_clip)\n",
    "augmented = augment_clip(waveform, TOTAL_LENGTH, rir_paths, noise_paths)\n",
    "\n",
    "print(f\"Original shape: {waveform.shape}\")\n",
    "print(f\"Augmented shape: {augmented.shape}\")\n",
    "\n",
    "# Save test augmentation\n",
    "test_aug_path = SCRIPT_DIR / \"test_augmented.wav\"\n",
    "torchaudio.save(str(test_aug_path), augmented.unsqueeze(0), 16000)\n",
    "display(Audio(str(test_aug_path), autoplay=False))\n",
    "print(f\"\\nTest augmented audio saved to: {test_aug_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Features\n",
    "\n",
    "Convert augmented audio to openWakeWord features and save as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import openwakeword feature computation\n",
    "from openwakeword.utils import AudioFeatures\n",
    "\n",
    "# Initialize feature extractor\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "feature_extractor = AudioFeatures(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_from_augmented_generator(generator, n_total: int, clip_duration: int,\n",
    "                                               output_file: str, feature_extractor):\n",
    "    \"\"\"Compute openWakeWord features from augmented audio generator.\"\"\"\n",
    "    \n",
    "    # Calculate output shape\n",
    "    # Features are computed every 1280 samples (80ms), starting after 12400 samples (775ms)\n",
    "    n_features_per_clip = (clip_duration - 12400) // 1280 + 1\n",
    "    feature_dim = 96  # openWakeWord embedding dimension\n",
    "    \n",
    "    # Create memory-mapped output file\n",
    "    output_shape = (n_total, n_features_per_clip, feature_dim)\n",
    "    mmap_file = open_memmap(output_file, mode='w+', dtype=np.float32, shape=output_shape)\n",
    "    \n",
    "    idx = 0\n",
    "    pbar = tqdm(total=n_total, desc=\"Computing features\")\n",
    "    \n",
    "    for batch in generator:\n",
    "        # batch is (batch_size, samples) as int16\n",
    "        for clip in batch:\n",
    "            if idx >= n_total:\n",
    "                break\n",
    "            \n",
    "            # Compute features for single clip\n",
    "            features = feature_extractor.embed_clips(clip[np.newaxis, :], batch_size=1)\n",
    "            \n",
    "            if features.shape[1] == n_features_per_clip:\n",
    "                mmap_file[idx] = features[0]\n",
    "                idx += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        mmap_file.flush()\n",
    "        \n",
    "        if idx >= n_total:\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Saved {idx} feature vectors to {output_file}\")\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if features already exist\n",
    "positive_train_features = MODEL_DIR / \"positive_features_train.npy\"\n",
    "positive_test_features = MODEL_DIR / \"positive_features_test.npy\"\n",
    "negative_train_features = MODEL_DIR / \"negative_features_train.npy\"\n",
    "negative_test_features = MODEL_DIR / \"negative_features_test.npy\"\n",
    "\n",
    "RECOMPUTE_FEATURES = False  # Set to True to recompute even if files exist\n",
    "\n",
    "all_exist = all(f.exists() for f in [\n",
    "    positive_train_features, positive_test_features,\n",
    "    negative_train_features, negative_test_features\n",
    "])\n",
    "\n",
    "if all_exist and not RECOMPUTE_FEATURES:\n",
    "    print(\"Feature files already exist. Set RECOMPUTE_FEATURES=True to regenerate.\")\n",
    "else:\n",
    "    print(\"Will compute features for all samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not positive_train_features.exists() or RECOMPUTE_FEATURES:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Computing positive training features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    clip_paths = list(POSITIVE_TRAIN_DIR.glob(\"*.wav\"))\n",
    "    n_total = len(clip_paths) * AUGMENTATION_ROUNDS\n",
    "    \n",
    "    generator = augment_clips_generator(\n",
    "        clip_paths, TOTAL_LENGTH, rir_paths, noise_paths,\n",
    "        batch_size=64, augmentation_rounds=AUGMENTATION_ROUNDS\n",
    "    )\n",
    "    \n",
    "    compute_features_from_augmented_generator(\n",
    "        generator, n_total, TOTAL_LENGTH,\n",
    "        str(positive_train_features), feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not negative_train_features.exists() or RECOMPUTE_FEATURES:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Computing negative training features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    clip_paths = list(NEGATIVE_TRAIN_DIR.glob(\"*.wav\"))\n",
    "    n_total = len(clip_paths) * AUGMENTATION_ROUNDS\n",
    "    \n",
    "    generator = augment_clips_generator(\n",
    "        clip_paths, TOTAL_LENGTH, rir_paths, noise_paths,\n",
    "        batch_size=64, augmentation_rounds=AUGMENTATION_ROUNDS\n",
    "    )\n",
    "    \n",
    "    compute_features_from_augmented_generator(\n",
    "        generator, n_total, TOTAL_LENGTH,\n",
    "        str(negative_train_features), feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not positive_test_features.exists() or RECOMPUTE_FEATURES:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Computing positive test features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    clip_paths = list(POSITIVE_TEST_DIR.glob(\"*.wav\"))\n",
    "    n_total = len(clip_paths) * AUGMENTATION_ROUNDS\n",
    "    \n",
    "    generator = augment_clips_generator(\n",
    "        clip_paths, TOTAL_LENGTH, rir_paths, noise_paths,\n",
    "        batch_size=64, augmentation_rounds=AUGMENTATION_ROUNDS\n",
    "    )\n",
    "    \n",
    "    compute_features_from_augmented_generator(\n",
    "        generator, n_total, TOTAL_LENGTH,\n",
    "        str(positive_test_features), feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not negative_test_features.exists() or RECOMPUTE_FEATURES:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Computing negative test features...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    clip_paths = list(NEGATIVE_TEST_DIR.glob(\"*.wav\"))\n",
    "    n_total = len(clip_paths) * AUGMENTATION_ROUNDS\n",
    "    \n",
    "    generator = augment_clips_generator(\n",
    "        clip_paths, TOTAL_LENGTH, rir_paths, noise_paths,\n",
    "        batch_size=64, augmentation_rounds=AUGMENTATION_ROUNDS\n",
    "    )\n",
    "    \n",
    "    compute_features_from_augmented_generator(\n",
    "        generator, n_total, TOTAL_LENGTH,\n",
    "        str(negative_test_features), feature_extractor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify feature files\n",
    "print(\"\\nFeature files:\")\n",
    "for f in [positive_train_features, positive_test_features, negative_train_features, negative_test_features]:\n",
    "    if f.exists():\n",
    "        data = np.load(str(f), mmap_mode='r')\n",
    "        print(f\"  {f.name}: shape={data.shape}\")\n",
    "    else:\n",
    "        print(f\"  {f.name}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Training Data\n",
    "\n",
    "Download pre-computed negative examples for training (diverse speech that's NOT your wake word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_LARGE_DOWNLOAD = False  # Set to True to skip the 16GB download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation features (small, always download)\n",
    "VAL_PATH = SCRIPT_DIR / \"validation_set_features.npy\"\n",
    "\n",
    "if not VAL_PATH.exists():\n",
    "    print(\"Downloading validation features...\")\n",
    "    url = \"https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "    with open(VAL_PATH, \"wb\") as f:\n",
    "        with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Validation features\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "else:\n",
    "    print(\"Validation features already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training features (large)\n",
    "FEATURES_PATH = SCRIPT_DIR / \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "\n",
    "if SKIP_LARGE_DOWNLOAD:\n",
    "    print(\"Skipping large feature download (training quality will be reduced)\")\n",
    "elif not FEATURES_PATH.exists():\n",
    "    print(\"Downloading training features (16GB, this will take a while)...\")\n",
    "    url = \"https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "    with open(FEATURES_PATH, \"wb\") as f:\n",
    "        with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Training features\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "else:\n",
    "    print(\"Training features already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Model\n",
    "\n",
    "Now we train using openwakeword's training script, but skip the augmentation step since we already computed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "N_STEPS = 10000\n",
    "FALSE_ACTIVATION_PENALTY = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load default config\n",
    "with open(OWW_DIR / \"examples\" / \"custom_model.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Modify config\n",
    "config[\"target_phrase\"] = [TARGET_WORD]\n",
    "config[\"model_name\"] = MODEL_OUTPUT_NAME\n",
    "config[\"output_dir\"] = str(OUTPUT_DIR)\n",
    "config[\"steps\"] = N_STEPS\n",
    "config[\"max_negative_weight\"] = FALSE_ACTIVATION_PENALTY\n",
    "config[\"target_accuracy\"] = 0.5\n",
    "config[\"target_recall\"] = 0.25\n",
    "\n",
    "# Data paths - use our pre-computed features\n",
    "config[\"false_positive_validation_data_path\"] = str(VAL_PATH)\n",
    "\n",
    "# Feature files for negative examples\n",
    "if FEATURES_PATH.exists():\n",
    "    config[\"feature_data_files\"] = {\"ACAV100M_sample\": str(FEATURES_PATH)}\n",
    "else:\n",
    "    config[\"feature_data_files\"] = {}\n",
    "\n",
    "# These won't be used since we skip generation/augmentation\n",
    "config[\"background_paths\"] = []\n",
    "config[\"rir_paths\"] = []\n",
    "config[\"n_samples\"] = N_SAMPLES_TRAIN\n",
    "config[\"n_samples_val\"] = N_SAMPLES_TEST\n",
    "\n",
    "# Save config\n",
    "CONFIG_YAML_PATH = SCRIPT_DIR / \"my_model.yaml\"\n",
    "with open(CONFIG_YAML_PATH, \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Config saved to: {CONFIG_YAML_PATH}\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Target word: {TARGET_WORD}\")\n",
    "print(f\"  Steps: {N_STEPS}\")\n",
    "print(f\"  False activation penalty: {FALSE_ACTIVATION_PENALTY}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training directly using openwakeword's Model class\n",
    "# This avoids the subprocess call and allows us to use our pre-computed features\n",
    "\n",
    "from openwakeword.train import Model\n",
    "from openwakeword.data import mmap_batch_generator\n",
    "\n",
    "# Get input shape from features\n",
    "input_shape = np.load(str(positive_test_features), mmap_mode='r').shape[1:]\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "# Create model\n",
    "oww_model = Model(\n",
    "    n_classes=1,\n",
    "    input_shape=input_shape,\n",
    "    model_type=config.get(\"model_type\", \"dnn\"),\n",
    "    layer_dim=config.get(\"layer_size\", 128),\n",
    "    seconds_per_example=1280 * input_shape[0] / 16000\n",
    ")\n",
    "\n",
    "print(\"Model created:\")\n",
    "oww_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data transform function for batch generation\n",
    "def transform_func(x, n=input_shape[0]):\n",
    "    \"\"\"Ensure data has correct shape for model.\"\"\"\n",
    "    if n > x.shape[1] or n < x.shape[1]:\n",
    "        x = np.vstack(x)\n",
    "        new_batch = np.array([x[i:i+n, :] for i in range(0, x.shape[0]-n, n)])\n",
    "    else:\n",
    "        return x\n",
    "    return new_batch\n",
    "\n",
    "# Setup data files\n",
    "feature_data_files = dict(config.get(\"feature_data_files\", {}))\n",
    "feature_data_files[\"positive\"] = str(positive_train_features)\n",
    "feature_data_files[\"adversarial_negative\"] = str(negative_train_features)\n",
    "\n",
    "# Data transforms\n",
    "data_transforms = {key: transform_func for key in feature_data_files.keys()}\n",
    "\n",
    "# Label transforms\n",
    "label_transforms = {}\n",
    "for key in feature_data_files.keys():\n",
    "    if key == \"positive\":\n",
    "        label_transforms[key] = lambda x: [1 for i in x]\n",
    "    else:\n",
    "        label_transforms[key] = lambda x: [0 for i in x]\n",
    "\n",
    "print(f\"Feature data files: {list(feature_data_files.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_generator = mmap_batch_generator(\n",
    "    feature_data_files,\n",
    "    n_per_class=config.get(\"batch_n_per_class\", {}),\n",
    "    data_transform_funcs=data_transforms,\n",
    "    label_transform_funcs=label_transforms\n",
    ")\n",
    "\n",
    "class IterDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    def __iter__(self):\n",
    "        return self.generator\n",
    "\n",
    "n_cpus = max(1, (os.cpu_count() or 1) // 2)\n",
    "X_train = torch.utils.data.DataLoader(\n",
    "    IterDataset(batch_generator),\n",
    "    batch_size=None,\n",
    "    num_workers=n_cpus,\n",
    "    prefetch_factor=16\n",
    ")\n",
    "\n",
    "# Validation data for false positives\n",
    "X_val_fp = np.load(str(VAL_PATH))\n",
    "X_val_fp = np.array([X_val_fp[i:i+input_shape[0]] for i in range(0, X_val_fp.shape[0]-input_shape[0], 1)])\n",
    "X_val_fp_labels = np.zeros(X_val_fp.shape[0]).astype(np.float32)\n",
    "X_val_fp_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X_val_fp), torch.from_numpy(X_val_fp_labels)),\n",
    "    batch_size=len(X_val_fp_labels)\n",
    ")\n",
    "\n",
    "# Validation data for accuracy/recall\n",
    "X_val_pos = np.load(str(positive_test_features))\n",
    "X_val_neg = np.load(str(negative_test_features))\n",
    "labels = np.hstack((np.ones(X_val_pos.shape[0]), np.zeros(X_val_neg.shape[0]))).astype(np.float32)\n",
    "\n",
    "X_val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(np.vstack((X_val_pos, X_val_neg))),\n",
    "        torch.from_numpy(labels)\n",
    "    ),\n",
    "    batch_size=len(labels)\n",
    ")\n",
    "\n",
    "print(f\"Training data: {len(feature_data_files)} sources\")\n",
    "print(f\"Validation FP data: {X_val_fp.shape}\")\n",
    "print(f\"Validation pos/neg data: {X_val_pos.shape[0]} pos, {X_val_neg.shape[0]} neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_model = oww_model.auto_train(\n",
    "    X_train=X_train,\n",
    "    X_val=X_val_loader,\n",
    "    false_positive_val_data=X_val_fp_loader,\n",
    "    steps=N_STEPS,\n",
    "    max_negative_weight=FALSE_ACTIVATION_PENALTY,\n",
    "    target_fp_per_hour=config.get(\"target_false_positives_per_hour\", 0.5),\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "oww_model.export_model(\n",
    "    model=best_model,\n",
    "    model_name=MODEL_OUTPUT_NAME,\n",
    "    output_dir=str(OUTPUT_DIR)\n",
    ")\n",
    "\n",
    "print(f\"\\nModel exported to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Done!\n",
    "\n",
    "Your trained model is in the `my_custom_model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTraining complete! Model files:\")\n",
    "if OUTPUT_DIR.exists():\n",
    "    for f in OUTPUT_DIR.glob(\"*\"):\n",
    "        if f.is_file():\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {f.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"  Output directory not found: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Copy to wakewords folder\n",
    "import shutil\n",
    "\n",
    "WAKEWORD_DIR = SCRIPT_DIR.parent / \"wakewords\" / TARGET_WORD.lower().replace(\" \", \"_\")\n",
    "WAKEWORD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for ext in [\".onnx\", \".tflite\"]:\n",
    "    src = OUTPUT_DIR / f\"{MODEL_OUTPUT_NAME}{ext}\"\n",
    "    if src.exists():\n",
    "        dst = WAKEWORD_DIR / f\"{MODEL_OUTPUT_NAME}{ext}\"\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"Copied {src.name} to {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "import openwakeword\n",
    "from openwakeword.model import Model as OWWModel\n",
    "\n",
    "# Load trained model\n",
    "model_path = OUTPUT_DIR / f\"{MODEL_OUTPUT_NAME}.onnx\"\n",
    "if model_path.exists():\n",
    "    oww = OWWModel(wakeword_models=[str(model_path)])\n",
    "    \n",
    "    # Test on a positive sample\n",
    "    test_clip = list(POSITIVE_TEST_DIR.glob(\"*.wav\"))[0]\n",
    "    audio, sr = torchaudio.load(str(test_clip))\n",
    "    audio = audio.squeeze().numpy()\n",
    "    \n",
    "    # Convert to int16\n",
    "    audio_int16 = (audio * 32767).astype(np.int16)\n",
    "    \n",
    "    # Run prediction\n",
    "    prediction = oww.predict(audio_int16)\n",
    "    print(f\"Test prediction on positive sample: {prediction}\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
